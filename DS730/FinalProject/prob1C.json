{"paragraphs":[{"text":"case class OshkoshWeather(Year: String, \nMonth: String, \nDay: String, \nTimeCST: String,\nTemperatureF: Integer, \nDewPointF: Integer, \nHumidity: Integer, \nSeaLevelPressure: Double, \nVisibilityMPH: Double, \nWindDirection: String, \nwindSpeedMPH: Double, \ngustspeedmph: Double, \nprecipitationIn: String, \nevents: String, \nconditions: String, \nwinddirdegrees: Integer)\n","user":"anonymous","dateUpdated":"2020-12-12T02:22:48+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class OshkoshWeather\n"}]},"apps":[],"jobName":"paragraph_1607647051615_1384946722","id":"20201211-003731_1951211646","dateCreated":"2020-12-11T00:37:31+0000","dateStarted":"2020-12-12T02:22:48+0000","dateFinished":"2020-12-12T02:22:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:306"},{"text":"val Oshkosh = spark.read.format(\"csv\").option(\"header\", true).option(\"inferSchema\", true).load(\"/user/maria_dev/final/Oshkosh/OshkoshWeather.csv\")\nOshkosh.createOrReplaceTempView(\"OshkoshWeather\")","user":"anonymous","dateUpdated":"2020-12-12T02:22:50+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Oshkosh: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 14 more fields]\n"}]},"apps":[],"jobName":"paragraph_1607647139992_676128690","id":"20201211-003859_777198448","dateCreated":"2020-12-11T00:38:59+0000","dateStarted":"2020-12-12T02:22:50+0000","dateFinished":"2020-12-12T02:22:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:307"},{"text":"\nspark.sqlContext.sql(\"select startday, date_add(startday, 6) as endday, avg(TemperatureF) from (select to_date(concat(string(year), '-', string(month), '-', string(day)), 'yyyy-m-d') as startday, myrange.* from oshkoshweather base join (select to_date(concat(string(year), '-', string(month), '-', string(day)), 'yyyy-m-d') as mydatetime, TemperatureF from oshkoshweather where TemperatureF != -9999) myrange on myrange.mydatetime between to_date(concat(string(base.year), '-', string(base.month), '-', string(base.day)), 'yyyy-m-d') and date_add(to_date(concat(string(base.year), '-', string(base.month), '-', string(base.day)), 'yyyy-m-d'), 6)) answer group by startday\").show()","user":"anonymous","dateUpdated":"2020-12-12T02:31:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1607647758854_-1815981358","id":"20201211-004918_443025494","dateCreated":"2020-12-11T00:49:18+0000","dateStarted":"2020-12-12T02:31:33+0000","dateFinished":"2020-12-12T02:31:30+0000","status":"RUNNING","progressUpdateIntervalMs":500,"$$hashKey":"object:308","errorMessage":""},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1607737714913_1568542457","id":"20201212-014834_771790231","dateCreated":"2020-12-12T01:48:34+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:309","text":"","dateUpdated":"2020-12-12T02:30:38+0000","dateFinished":"2020-12-12T02:29:38+0000","dateStarted":"2020-12-12T02:28:08+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job 31 cancelled part of cancelled job group zeppelin-20201212-014834_771790231\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1539)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:811)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1789)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3272)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:723)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:682)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:691)\n  ... 46 elided\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1607739852887_-1870293427","id":"20201212-022412_412620887","dateCreated":"2020-12-12T02:24:12+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1501"}],"name":"FinalPartC","id":"2FUYH7UBZ","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}