In 1992, three Staten Island musicians and cousins named Robert Diggs, Gary Grice, and Russell Jones were finding a following in New York, but commercial success eluded them. They adapted, changing their performing names and formed a new group based around a mutual love of old Kung-fu movies. Over the next year, they expanded to nine members and used the momentum from their independently released single to sign to a major label. In November 1993, GZA, Ol’ Dirty Bastard, Method Man, Raekwon, Ghostface Killah, Inspectah Deck, U-God and Masta Killa, collectively known as the Wu-Tang Clan, released their debut album Enter The Wu-Tang (36 Chambers). (Erlewine, n.d.) Hip-hop’s founders had adopted aliases to perform under, so the Wu-Tang was merely continuing that tradition. But this was the first time there was an element of another culture was so completely woven in. Enter The Wu-Tang sampled heavily from Kung fu movies, and even based the album’s title on 1973’s Enter The Dragon. Thematically, using Kung-fu battles as a metaphor for both their life in New York City’s streets and their lyrical prowess worked with GZA’s production to tie together the nine different rappers together to create one of the defining albums of the 90’s. 
Pseudonyms have been a part of American culture since Samuel Clemens was on a boat in 12 feet of water. Comic book superheroes protect their identities by wearing masks and keep their real name hidden. President Richard Nixon was forced to resign when FBI Associate Director Mark Felt told Washington Post reporters Woodward and Bernstein about the break in at the Watergate hotel, but Americans only knew him by his code name “Deep Throat.” 
Since home computers were available to connect to bulletin board services, the idea of a user “verifying” themselves was born. Early verification was simply to prove you were not a lamer or law enforcement and were worthy of having access to cracked software and ways to make free long-distance calls. Until Twitter introduced blue checkmarks signifying an account accurately depicts who controls it in 2009, online verification never got more advanced than that. Message boards, IRC rooms, and AOL email addresses were only concerned with having a unique username, not that it had any relation to the user.
Is that a mistake? Should online communication be directly tied to an individual? Does everyone need a blue checkmark? Clearly, the capacity and appetite are there if Twitter has already implemented the idea. After foreign interference via social media possibly influenced the 2016 Presidential election, the idea is worth discussing. The governments of Australia and New Zealand have implemented ways to tie online accounts to government records for accessing wait lists for senior care facilities, pay taxes, and report on their business’s gender equality, so at least some parts of the world feel confident in the technology to function as intended. (New Zealand Government, 2020) (Australian Government, n.d.) 
There would be other benefits to online communities as well. Nobody could impersonate someone else. If a user violates the agreements of an online forum, any punishment would be tied to the individual, preventing that person from creating a new account to regain access. “Doxxing” would take on a form of mutually assured destruction, as anyone sharing other people’s personal information could not hide behind their own anonymity to protect against retribution. 
On the other hand, tying online identities to government-issued identification would make surveillance of speech online incredibly easy. Other people could see huge amounts of data on an individual, and pick and choose personal attacks in their replies. Additionally, “verified accounts” mean nothing if someone else gains access to them. For example, recently activists have “taken over” celebrities social media accounts to use their significant follower count to amplify messages like how to support the Black Lives Matter movement. Even more recently, some of the most-followed Twitter accounts were hacked, leading to a few hours where nobody verified could tweet. (Kovach, 2020)
Finally, it would do away with several avenues of creativity. Two sportswriters went by aliases, Celebrity Hot Tub and Ken Tremendous, when they started writing online to avoid raising any eyebrows at their day jobs about what they wrote in their free time. Celebrity Hot Tub, real name Ryan Nanni, stopped practicing law to write full time, and ended up becoming an editor-in-chief. Ken Tremendous, real name Michael Schur, has won two Emmy’s for his comedic writing that had nothing to do with sports. (Leitch, 2008) Additionally, there are other online personas that would lose a tremendous amount of magic if someone were to identify them. For example, @OldHossRadbourn tweets in the style of an 1800’s-era baseball pitcher, and @darth is a red panda that wears Darth Vader-style armor that loves all kinds of potatoes and pets. These accounts are so committed to their persona that they become like a team’s mascot. It is not important who they are in real life, it is what they represent. 
	If we view online communities as their own culture, it would be prudent to look at fake identities through the lens of ethical relativism. First, the ability to create your own person online is available to anyone in that space, or available to nobody. Twitter, Facebook, and Instagram are all popular platforms where you can create your own persona. Compare that to Canvas’ message boards, or work-associated platforms like Salesforce, Workday, or Teams, where the administrator creates the account that you use when you join that organization.
	Joining a new company or enrolling in a new school is a sort of fresh start, and thinking of how we start new relationships in social circles helps underscore the differences in websites where you can or can’t create your own identity. Those first days are primarily telling your new connections about your hobbies, your family, where you grew up. If those interactions happen to be online (as are happening more and more), it is the same sort of patter. We share just enough information to form a connection without revealing too much at once. Compare this to social media or message boards that have anonymous identities, and anyone sharing that much of their home life right away would be sticking out as overly self-centered. Given that some variation of “speech” is all that we can do online, ethical relativism emphasizes “reading the room” to figure out how and what to communicate, both about the topic at hand and about the self. 
	That speech-only communication can make fake online identities freeing. To help understand why, as a counterexample take punk rock aesthetics. The 1970’s to 1980’s countercultural movement championed by bands like The Ramones, Sex Pistols, and The Clash was hallmarked by spiked and neon hair, numerous piercings, and leather or denim jackets adorned with metal. All those elements meant punk rockers could not only be instantly recognized but had a high investment into their culture. There was no fast fashion that made cheap leather jackets: buying one and then modifying it showed a serious commitment. Neon hairstyles or partially shaved heads were not done for Friday and Saturdays, because they were someone’s hairstyle the rest of the week too. To really fit in to the punk lifestyle, you had to be dedicated. Compare that to signing up for a punk rock message board that requires a free email address. 
	There is a sense of freedom in having no stakes attached to your identity. Walking the streets of Manhattan alone, with millions of people around you that don’t know you, has a powerful sense of isolation. Anyone can replicate that online by limiting what you choose to share, but adapting a fake personality can also be limiting. For example, Eric Sollenberger has written under the alias PFT Commenter for eight years, taking the personality of the average person that posts on an NFL-themed news site Pro Football Talk with a deeply satirical bent. (Wagner, 2018) His Twitter account became very popular, and he ended up writing for bigger and bigger sports sites. Eventually, he landed his own podcast, filmed an episode of an ESPN show, and has done countless interviews with national publications, all under the guise of his online handle based around being a random internet user. Most “random internet users” are not on ESPN. Would Sollenberger, who developed into a sought-after media personality, had the same outcome if he did not have his backstory as a hook? 
	Utilitarianism might have some ideas on that. In Sollenberger’s case, if his pseudonym aided in his popularity and led to more lucrative gigs due to more people finding and enjoying his work, that is a true positive all around. What about the inverse, where someone famous wants to remain anonymous? Surprisingly, it can and does happen. For example, Mitt Romney created a Twitter account mainly to curate news feeds he was interested in, and occasionally responding to point out what he did as a politician. (Feinberg, 2019) The reporters he commented on might have appreciated knowing they caught the eye of a senator, but that’s probably the reason why Romney did it – to give himself a shred of privacy. Now, saying “I have a private account” in The Atlantic is not the best way to keep it under wraps, as the fact it’s being written about in a paper about online identities shows, but overall utilitarianism would consider it at least beneficial to a sitting U.S. Senator. 
The idea of keeping your online identity secret turns dangerous is when that air of mystery combines with dangerous rhetoric and leads to real-life consequences. American politics and elections have had to deal with fake accounts disseminating wrong information. During the 2016 presidential election, Democratic email accounts were hacked by a Russian foreign intelligence agency. To avoid any collusion charges of acting with a political party, the Russian team “laundered” the information through a neutral site, Wikileaks. Additionally, that same team used fake accounts to gain acceptance to politically motivated groups, sharing fake news articles to push that group to a further extreme. Some of their accounts, like @jenn_abrams, became so popular posts under that name were citied by BuzzFeed and the New York Times. (Collins & Cox, 2017)
An ongoing hidden identity is Q, an entity spreading conspiracy theories about a “deep state” plot to bring down Donald Trump and his supporters. People believe that Q is the only one sharing information about the secret battle between Trump and some combination of Democratic politicians, Hollywood actors, and others prominent figures, depending on the flavor of the month. Here, not knowing Q’s real identity allows his followers, who refer to themselves as QAnon, to have the belief Q could know these “secret plans.” Unlike smaller cults of earlier years, enough people believe these conspiracy theories, and have acted to “stop” them, the FBI has labeled the group a domestic terrorism threat. (Bump, 2019)
Both examples rely on online anonymity to amplify the spread of disinformation to disrupt American politics. While that concept is nothing new, there is a new danger in doing so in some Internet communities. One of the great ironies of modern communication is that we can get new information from anywhere, but the algorithms that recommend content on Facebook, Twitter and YouTube show content that will keep users engaged to sell more ads. If those recommendations are political, they end up being a little more extreme than what the user just experienced. As that repeats, these fringe positions end up being served to the most receptive audience. 
Based on these examples, utilitarianism would seem ambivalent on the morality of using fake accounts in an online community. If anything, it seems like using a fake account is more like a tool. Anonymity is a choice in a lot of situations offline as well, be it paying for someone else’s food or driving away after hitting a parked car. What if the focus is shifted onto the owner of each online community? If a core of good software is maintenance, shouldn’t that idea hold for how users interact with the software, even if that software is designed so users interact? We can examine the owner – user dynamic better by looking at parts of the Association for Computing Machinery’s Code of Ethics and applying them to online communities, especially on how users access them. (Association for Computing Machinery, 2018)
Right away, “Avoid harm” forces community administrators to wonder if they have a responsibility to deal with one user harming another in some form. Nobody can control what an individual posts or shares, but there are ways to curb that behavior. For example, having written rules of behavior that is not tolerated, along with the ability to enforce them through actions, is straightforward. In fact, by following two more principles by “being honest and trustworthy” and “taking action not to discriminate,” online community leaders can find that users will help call out behavior that’s disrupting, since they know that little effort means their experience will improve. 
No matter what the rules are, they all run into the same issue: how do you have any sort of punishment in an anonymous environment? Creating new accounts is streamlined to be as painless as possible, so banning one account is almost meaningless. There are a few options that are reasonable, fair, and have adjustable levels of anonymity. An easy way to stop someone from signing up repeatedly is to charge them. At the niche end there are businesses that build online social clubs based around a specific topic, pairing them with online courses to help develop a skill like gardening or blogging. There, the membership fee presents real consequences for violating online etiquette rules. Other “general info” communities may have a fee to sign up or have additional forums only accessible to those paying. But, as painless as sending money online is, it does require a credit card or bank account, which is not anonymous. An alternative might be making posting more difficult to being with. For example, new accounts cannot participate until after a week, or any post requires completing some reCAPTCHA authentications. If users don’t want to pay in cash, they can pay in time if community founders take a literal view of the principles “respect privacy” and “honor confidentiality.” 
	Finally, both organizers and users of online community should understand what “respect the work required to produce new ideas” means in their space. On the individual level, sharing art, writing, breakthroughs, and ideas has never been easier, but producing those works takes time and effort. If you want to create a community where people want to share their talents, it can’t have people taking a third of a minute to insult it. In a meta sense, building that kind of community is itself creative work. Ensuring that work thrives is up to everyone involved. 
	The general ethical principles listed under the ACM’s Code of Ethics apply nicely to the idea of online computing, but the professional and leadership responsibilities are not as applicable, with one exception. “Recognize and take special care of systems that become integrated into the infrastructure of society” is an idea that has only very, very recently has started being addressed by the largest tech companies. For example, searching for “Twitter QAnon” has a link to Twitter’s QAnon hashtag, search, along with news articles about Twitter banning 7,000 accounts linked to the group the FBI describes as a domestic terrorist organization. Searching for “Facebook QAnon” shows several Facebook groups about QAnon organizations, and a New York Times article about Facebook’s internal discussions around banning political advertising on the site until the November elections. These are not new problems for the platforms, but the pressure to effectively moderate content on them is new. 
	The same ways that fake or anonymous accounts can be beneficial in exploring aspects of someone’s identity can also greatly harm communities, both online and in real life. Looking at the examples of how they brought harm through the eyes of online community organizers helped clarify that a lot of the issue caused is from two kinds of asymmetrical information distributions. One is obvious – Russian intelligence agencies and Q know who they really are but lie about their identity to further their goals. The other is that the platforms they used are filled with people that use their real identity. Compare the Data Science 760 discussion boards to a Facebook comment thread or Twitter trend, and they will all have plenty of real names attached to the accounts. The difference is that for our discussion boards, they are tied to the University of Wisconsin system, so posts are coming from that person. On other platforms, people used their real names to sign up if they wanted to. There are enough people using real accounts that users mentally believe that everyone uses real accounts, so bad actors are never given any vetting. 
	For those communities, the best solution might be adding to Twitter’s verification system. Currently, blue checkmarks appear next to usernames that have been authenticated via Twitter employees. But there are other ways to automate a “soft” verification. For example, two-factor authentication via a cell phone is a common way to ensure whoever is logging into an account is the true account owner. Website owners can use that phone number as a vote of confidence to some level of authenticity, especially if that idea is paired with reverse phone number lookup to see who has registered that number. Other details like how long an account has been registered, number of times it has been reported by other users, and typical account interactions could all be folded into another colored checkmark. The would be some initial shock to a community as it sorts out this “caste system” of verification, but it would be help encourage people to think about who they are interacting with. 
	 Would that be enough? If fake accounts online helped change who was elected to office or cause enough damage in the real world to be labeled as a terrorist threat, some could argue that more concrete steps need to be taken. Social media that becomes relied on as heavily as Facebook and Twitter could actively verify all users. There is certainly precedent for this. Using these platforms is purely optional. Tying a person’s online identity to a government identity could be a requirement of using a platform. Verifying someone’s identity is a normal routine for many aspects of life, like boarding a plane, giving blood, or buying alcohol. If Australia and New Zealand feel confident enough in the technology to verify a user’s real identification, why not social media companies that become successful enough to be relied on as forms of mass communication by hundreds of millions of users? 
	There are two big issues that stem from bringing the government into this. First, there is no “the government.” Each nation has different ways to register a citizen and requiring websites to learn each one and create a safe and accurate process using each one is a tall ask. Australia and New Zealand’s authentication gets around that issue by limiting the audience to their respective countries. They aren’t running social media websites, they want people to pay their taxes and file compliance information. If a nation only requires this sort of authentication by the biggest user bases like Twitter and Facebook, we hit a snag. The growth of each website’s user base happened without authentication. Then, at some point, new users would be required to authenticate to sign up, creating a new class of users. Or, all existing users would have to authenticate. Either situation still faces the second issue: governments getting involved in speech. Using government resources to register online is not very far away from governments tracking what people say online. Allowing elected officials to be able to identify their political opposition is as horrible an idea spreading conspiracy theories about how much surveillance a government places on its citizens. 
	The biggest reasons why a tech company would not want to work on limiting speech are moderation and moderate. Small communities with established rules can have their own moderation team to help curtail hateful, demeaning, or disruptive speech. But the largest tech companies wanted to grow quickly rather than have moderate success. They invested in scale to get investors, rather than in community leaders to get better. Only recently have the biggest companies taken the smallest steps to regulate what happens on their platforms. Where they are running into problems is because their noble goal of “attempt to regulate speech” requires trained humans to fully understand what goes on in those conversations. Moderating speech on the global scale that Facebook and Twitter is the reaping to what they’ve sowed. They used technology’s ability to replicate quickly and watched their creation spread out of their control. 
Works Cited
Association for Computing Machinery. (2018, June 22). ACM Code of Ethics and Professional Conduct. Retrieved from Association for Computing Machinery: https://www.acm.org/code-of-ethics
Australian Government. (n.d.). my GovID. Retrieved from https://www.mygovid.gov.au/
Bump, P. (2019, August 2). Hours after an FBI warning about QAnon is published, a QAnon slogan turns up at Trump’s rally. Retrieved from Washington Post: https://www.washingtonpost.com/politics/2019/08/02/hours-after-an-fbi-warning-about-qanon-is-published-qanon-slogan-turns-up-trumps-rally/
Collins, B., & Cox, J. (2017, November 3). Jenna Abrams, Russia’s Clown Troll Princess, Duped the Mainstream Media and the World. Retrieved from The Daily Beast: https://www.thedailybeast.com/jenna-abrams-russias-clown-troll-princess-duped-the-mainstream-media-and-the-world
Erlewine, S. T. (n.d.). Wu-Tang Clan | Biography & History. Retrieved from All Music: https://www.allmusic.com/artist/wu-tang-clan-mn0000959876/biography
Feinberg, A. (2019, October 20). This Sure Looks Like Mitt Romney’s Secret Twitter Account (Update: It Is). Retrieved from Slate: https://slate.com/news-and-politics/2019/10/mitt-romney-has-a-secret-twitter-account-and-it-sure-looks-like-its-this-one.html
Kovach, S. (2020, July 16). Twitter was ill-equipped to handle an unprecedented hack — now we need answers. Retrieved from CNBC: https://www.cnbc.com/2020/07/16/twitter-hack-how-hackers-gained-access-to-accounts.html
Leitch, W. (2008, Feb 7). One Of Our Favorite Sports Bloggers Is ... Mose Schrute?! Retrieved from Deadspin: https://deadspin.com/one-of-our-favorite-sports-bloggers-is-mose-schrute-353709
New Zealand Government. (2020, Feb 5). What is RealMe? Retrieved from New Zealand Government: https://www.govt.nz/browse/passports-citizenship-and-identity/proving-and-protecting-your-identity/what-is-realme/
Wagner, L. (2018, May 22). So Who Is Barstool's PFT Commenter? Retrieved from Deadspin: https://deadspin.com/so-who-is-barstools-pft-commenter-1826203697


